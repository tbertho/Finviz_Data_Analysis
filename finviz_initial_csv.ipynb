{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Code beta mit einem Ticker\n",
    "\n",
    "path = 'C:/Users/trist/code/Data/Finviz_insider/AMZN.csv'\n",
    "\n",
    "root = \"https://finviz.com/quote.ashx?t=\"\n",
    "ticker = \"AMZN\"\n",
    "date_format = \"%d.%m.%Y\"\n",
    "\n",
    "url = root + ticker\n",
    "\n",
    "def crawl(url):\n",
    "    req = Request(url = url, headers={\"user-agent\": \"my-app\"})\n",
    "    response = urlopen(req)\n",
    "\n",
    "    html = BeautifulSoup(response, 'html')\n",
    "    insider_table = html.find(class_=\"body-table insider-trading-table\")\n",
    "\n",
    "    parsed_data = []\n",
    "   \n",
    "    for row in insider_table.find_all(\"tr\")[1:]:\n",
    "        insider_name = row.find('td')\n",
    "        # Get the Tradername in the first td element\n",
    "        if insider_name is not None:\n",
    "            insider_name = insider_name.text.strip()\n",
    "        else:\n",
    "            insider_name = None\n",
    "        # Get the title of the Insider in the second td element\n",
    "        insider_title = row.find_all('td')[1]\n",
    "        if insider_title is not None:\n",
    "            insider_title = insider_title.text.strip()\n",
    "        else:\n",
    "            insider_title = None\n",
    "        # Get the date of the transaction in the third td element\n",
    "        transaction_date = row.find_all('td')[2]\n",
    "        if transaction_date is not None:\n",
    "            transaction_date = transaction_date.text.strip()\n",
    "        else:\n",
    "            transaction_date = None\n",
    "        # Get the type of transaction in the fourth td element\n",
    "        transaction_type = row.find_all('td')[3]\n",
    "        if transaction_type is not None:\n",
    "            transaction_type = transaction_type.text.strip()\n",
    "        else:\n",
    "            transaction_type = None\n",
    "        # Get the Cost of transaction in the fifth td element\n",
    "        cost = row.find_all('td')[4]\n",
    "        if cost is not None:\n",
    "            cost = cost.text.strip()\n",
    "        else:\n",
    "            cost = None\n",
    "        # Get the num of shares in the sixth td element\n",
    "        num_shares = row.find_all('td')[5]\n",
    "        if num_shares is not None:\n",
    "            num_shares = num_shares.text.strip()\n",
    "        else:\n",
    "            num_shares = None\n",
    "        # Get the Value of shares in the seventh td element\n",
    "        value_shares = row.find_all('td')[6]\n",
    "        if value_shares is not None:\n",
    "            value_shares = value_shares.text.strip()\n",
    "        else:\n",
    "            value_shares = None\n",
    "        # Get the num of shares the insider owns in the eighth td element\n",
    "        total_num_shares_insider_owns = row.find_all('td')[7]\n",
    "        if total_num_shares_insider_owns is not None:\n",
    "            total_num_shares_insider_owns = total_num_shares_insider_owns.text.strip()\n",
    "        else:\n",
    "            total_num_shares_insider_owns = None\n",
    "        # Get the date of the SEC Form, ka was das ist,  the ninth td element\n",
    "        SEC_date = row.find_all('td')[7]\n",
    "        if SEC_date is not None:\n",
    "            SEC_date = SEC_date.text.strip()\n",
    "        else:\n",
    "            SEC_date = None\n",
    "\n",
    "        parsed_data.append([insider_name, insider_title, transaction_date, transaction_type,\n",
    "                            cost, num_shares, value_shares, total_num_shares_insider_owns, SEC_date])\n",
    "    return parsed_data\n",
    "    \n",
    "def create_initial_df(path):\n",
    "\n",
    "    list = crawl(url)\n",
    "    df = pd.DataFrame(list, columns = [\"insider\" , \"title\", \"transaction_date\", \"transaction_type\",\n",
    "                            \"cost\", \"num_shares\", \"value_shares\", \"total_num_shares_insider_owns\", \"SEC_date\"])\n",
    "    #save df to local storage\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "create_initial_df(path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
